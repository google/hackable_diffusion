{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training MNIST using simplicial diffusion ðŸš€\n",
        "\n",
        "In this colab we showcase how to train a **simplicial** diffusion model on MNIST dataset. This colab can run on any colab backend."
      ],
      "metadata": {
        "id": "K43uX-gRu1O8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLydYq_yuo2I"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Common modules\n",
        "################################################################################\n",
        "\n",
        "import dataclasses\n",
        "import functools\n",
        "from etils import ecolab\n",
        "import flax.linen as nn\n",
        "import grain.python as pygrain\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow_datasets as tfds\n",
        "import tqdm\n",
        "\n",
        "################################################################################\n",
        "# Hackable diffusion modules\n",
        "################################################################################\n",
        "\n",
        "cell_autoreload = True  # @param{type: \"boolean\"}\n",
        "\n",
        "with ecolab.adhoc(\n",
        "    reload=[\"hackable_diffusion\"],\n",
        "    invalidate=False,\n",
        "    cell_autoreload=cell_autoreload,\n",
        "):\n",
        "  from hackable_diffusion import hd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diffusion_network = hd.diffusion_network\n",
        "time_sampling = hd.time_sampling\n",
        "simplicial = hd.corruption.simplicial\n",
        "schedules = hd.corruption.schedules\n",
        "arch_typing = hd.architecture.arch_typing\n",
        "conditioning_encoder = hd.architecture.conditioning_encoder\n",
        "discrete_backbone = hd.architecture.discrete\n",
        "simplicial_backbone = hd.architecture.simplicial\n",
        "unet = hd.architecture.unet\n",
        "wrappers = hd.inference.wrappers\n",
        "diffusion_inference = hd.inference.diffusion_inference\n",
        "discrete_loss = hd.loss.discrete\n",
        "simplicial_step_sampler = hd.sampling.simplicial_step_sampler\n",
        "sampling = hd.sampling.sampling\n",
        "time_scheduling = hd.sampling.time_scheduling"
      ],
      "metadata": {
        "id": "bnCZLUolDmbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare MNIST data"
      ],
      "metadata": {
        "id": "_e8om2QJvWDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create py-grain data structures for convenient batching and loading.\n",
        "\n",
        "MNIST data is $28 \\times 28 \\times 1$."
      ],
      "metadata": {
        "id": "kAeUl5SQpdGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclasses.dataclass(frozen=True)\n",
        "class PreprocessExample(pygrain.MapTransform):\n",
        "  \"\"\"Preprocesses an example.\"\"\"\n",
        "\n",
        "  def map(self, x: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
        "    \"\"\"Converts everything to int32.\"\"\"\n",
        "\n",
        "    image = x['image'].astype(np.int32)\n",
        "    image = np.reshape(image, (28, 28, 1))\n",
        "    # We add additional dimension for the tokens\n",
        "    image = np.expand_dims(image, axis=-1)\n",
        "\n",
        "    return {\n",
        "        'data': image,\n",
        "        'label': np.int32(x['label']),\n",
        "    }\n",
        "\n",
        "\n",
        "def mnist_dataset(batch_size, train) -> pygrain.DataLoader:\n",
        "  loader = pygrain.load(\n",
        "      source=tfds.data_source(name='mnist', split='all'),\n",
        "      shuffle=True if train else False,\n",
        "      shard_options=pygrain.ShardByJaxProcess(drop_remainder=True),\n",
        "      transformations=[PreprocessExample()],\n",
        "      batch_size=batch_size,\n",
        "      drop_remainder=True,\n",
        "      seed=0,\n",
        "  )\n",
        "  return loader"
      ],
      "metadata": {
        "id": "790dD3J2o5Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_plot_images = next(iter(mnist_dataset(64, train=False)))['data']\n",
        "fig, axes = plt.subplots(8, 8, figsize=(8, 8))\n",
        "for img, ax in zip(mnist_plot_images[:64], axes.flatten()):\n",
        "  ax.imshow(img[:, :, :, 0])\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6nYspS7cpM6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(linewidth=160)\n",
        "mnist_plot_images[0][:, :, 0, 0]"
      ],
      "metadata": {
        "id": "SpSzH10oZDtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define all diffusion model modules"
      ],
      "metadata": {
        "id": "OWP4NiIfMBde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noise process"
      ],
      "metadata": {
        "id": "xN5dcCQhLRSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use cosine discrete schedule"
      ],
      "metadata": {
        "id": "odHo5roqFfYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schedule = schedules.LinearDiscreteSchedule()\n",
        "process = simplicial.SimplicialProcess.uniform_process(\n",
        "    schedule=schedule, num_categories=256\n",
        ")"
      ],
      "metadata": {
        "id": "doItCfTdLQvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize noise process"
      ],
      "metadata": {
        "id": "SDjo5LRSLskH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_noises = 7\n",
        "fig, axes = plt.subplots(\n",
        "    ncols=num_noises, figsize=(num_noises * 4, 4), sharex=True, sharey=True\n",
        ")\n",
        "\n",
        "corrupt_rng = jax.random.PRNGKey(10)\n",
        "idx = 0\n",
        "for time in jnp.linspace(1e-3, 1.0 - 1e-3, num=num_noises):\n",
        "  xt, targets = process.corrupt(\n",
        "      key=corrupt_rng,\n",
        "      x0=jnp.array(mnist_plot_images),\n",
        "      time=jnp.ones((1,)) * time,\n",
        "  )\n",
        "  ax = axes[idx]\n",
        "  xt_amax = jnp.argmax(xt[0, ...], axis=-1)\n",
        "  ax.imshow(xt_amax)\n",
        "  ax.axis('off')\n",
        "  ax.set_title(f'Time = {time}')\n",
        "  idx += 1"
      ],
      "metadata": {
        "id": "12mocRWzLuAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define diffusion network backbone\n",
        "\n",
        "First, we define diffusion backbone -- an architecture which takex `x` and `conditioning_embeddings`, as well as `is_training` and returns the same type as `x`.\n",
        "\n",
        "Here, we use a small version of `Unet`."
      ],
      "metadata": {
        "id": "8fiOvaxNMOh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_backbone = unet.Unet(\n",
        "    base_channels=32,\n",
        "    channels_multiplier=(1, 2, 2),\n",
        "    num_residual_blocks=(2, 2, 2),\n",
        "    downsample_method=arch_typing.DownsampleType.AVG_POOL,\n",
        "    upsample_method=arch_typing.UpsampleType.NEAREST,\n",
        "    dropout_rate=(0.0, 0.0, 0.2),\n",
        "    bottleneck_dropout_rate=0.2,\n",
        "    self_attention_bool=(False, False, False),\n",
        "    cross_attention_bool=(False, False, False),\n",
        "    attention_normalize_qk=False,\n",
        "    attention_use_rope=False,\n",
        "    attention_rope_position_type=arch_typing.RoPEPositionType.SQUARE,\n",
        "    attention_num_heads=8,\n",
        "    attention_head_dim=-1,\n",
        "    normalization_type=arch_typing.NormalizationType.RMS_NORM,\n",
        "    normalization_num_groups=None,\n",
        "    zero_init_output=False,\n",
        "    activation='gelu',\n",
        "    skip_connection_method=arch_typing.SkipConnectionMethod.NORMALIZED_ADD,\n",
        ")\n",
        "\n",
        "backbone = simplicial_backbone.ConditionalSimplicialBackbone(\n",
        "    base_backbone=base_backbone,\n",
        "    logit_embedder=simplicial_backbone.DenseEmbedder(\n",
        "        embedding_dim=32,\n",
        "        adapt_to_image_like_data=True,\n",
        "    ),\n",
        "    logit_projector=discrete_backbone.DenseProjector(\n",
        "        embedding_dim=32,\n",
        "        num_categories=process.process_num_categories,\n",
        "        adapt_to_image_like_data=True,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "11x4iOOprtTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define conditioning logic"
      ],
      "metadata": {
        "id": "ARc5ksUEHG0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we define the conditioning embedders as well as the time encoder. The conditioning encoder processes each conditioning (in the case of MNIST data, each batch comes with its label (`label`)).\n",
        "\n",
        "The conditioning encoder is a dictionary with key `label` (and here the value is a `nn.Module` which is given by a simple `LabelEmbedding` module). If you want to train a purely unconditional model, set `conditioning_embedders = {}`.\n",
        "\n"
      ],
      "metadata": {
        "id": "fqGG4Co-HD1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Conditional diffusion.\n",
        "################################################################################\n",
        "\n",
        "conditioning_embedders = {\n",
        "    'label': conditioning_encoder.LabelEmbedder(\n",
        "        num_classes=10,\n",
        "        num_features=256,\n",
        "        conditioning_key='label',\n",
        "    )\n",
        "}\n",
        "\n",
        "encoder = conditioning_encoder.ConditioningEncoder(\n",
        "    time_embedder=conditioning_encoder.SinusoidalTimeEmbedder(\n",
        "        activation='gelu', embedding_dim=256, num_features=256\n",
        "    ),\n",
        "    conditioning_embedders=conditioning_embedders,\n",
        "    embedding_merging_method=arch_typing.EmbeddingMergeMethod.SUM,\n",
        "    conditioning_rules={\n",
        "        'time': arch_typing.ConditioningMechanism.ADAPTIVE_NORM,\n",
        "        'label': arch_typing.ConditioningMechanism.ADAPTIVE_NORM,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "pDQv0FEEHFj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting all together into diffusion network"
      ],
      "metadata": {
        "id": "E9Cy-hgrH7gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = diffusion_network.DiffusionNetwork(\n",
        "    backbone_network=backbone,\n",
        "    conditioning_encoder=encoder,\n",
        "    prediction_type='logits',\n",
        "    data_dtype=jnp.float32,\n",
        ")"
      ],
      "metadata": {
        "id": "7EUVYISMH9bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model visualization"
      ],
      "metadata": {
        "id": "5rwfPG_EtTS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_depth = 2  # @param {type: \"integer\"}\n",
        "\n",
        "tabulate_fn = nn.tabulate(\n",
        "    network,\n",
        "    jax.random.PRNGKey(42),\n",
        "    depth=summary_depth,\n",
        "    console_kwargs={\"force_jupyter\": True, \"soft_wrap\": True},\n",
        ")\n",
        "\n",
        "dummy_time = jnp.ones((1,))\n",
        "dummy_xt = jax.nn.one_hot(\n",
        "    jnp.ones((1, 28, 28, 1)), process.process_num_categories\n",
        ")\n",
        "dummy_conditioning = {\"label\": jnp.ones((1,), dtype=jnp.int32)}\n",
        "\n",
        "print(\n",
        "    tabulate_fn(\n",
        "        dummy_time,\n",
        "        dummy_xt,\n",
        "        dummy_conditioning,\n",
        "        is_training=False,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "LvX6WLQGN1cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define time sampler, optimizer and loss function\n",
        "\n",
        "The time is sampled uniformly in the interval $[\\epsilon,1 - \\epsilon]$.\n",
        "\n",
        "The loss is simply the $\\ell_2$ loss."
      ],
      "metadata": {
        "id": "emGbkd8vMWyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_sampler = time_sampling.UniformTimeSampler(safety_epsilon=1e-3)\n",
        "\n",
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(max_norm=1.0),\n",
        "    optax.scale_by_adam(b1=0.9, b2=0.999, eps=1e-8),\n",
        "    optax.scale_by_schedule(optax.constant_schedule(value=5e-4)),\n",
        "    optax.scale(-1.0),\n",
        ")\n",
        "\n",
        "loss_fn = discrete_loss.DiffusionCrossEntropyLoss(\n",
        "    schedule=schedule, weight_fn=None\n",
        ")"
      ],
      "metadata": {
        "id": "1Y-S0iHuIa9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the parameters loss function and gradient function\n",
        "\n",
        "Here we define the loss function as well as gradient function to be dependent on NN parameters. This is needed for training the neural network."
      ],
      "metadata": {
        "id": "qYyGqTogMdp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def params_loss_fn(params, x0, conditioning, rng):\n",
        "  time_rng, corrupt_rng = jax.random.split(rng, 2)\n",
        "  time = time_sampler(key=time_rng, data_spec=x0)\n",
        "  xt, targets = process.corrupt(key=corrupt_rng, x0=x0, time=time)\n",
        "  output = network.apply(\n",
        "      {'params': params},\n",
        "      time=time,\n",
        "      xt=xt,\n",
        "      conditioning=conditioning,\n",
        "      is_training=True,\n",
        "      rngs={'dropout': rng},\n",
        "  )\n",
        "  out = jnp.mean(loss_fn(preds=output, targets=targets, time=time))\n",
        "  return out, {'loss': out}\n",
        "\n",
        "\n",
        "grad_fn = jax.jit(jax.grad(params_loss_fn, has_aux=True))"
      ],
      "metadata": {
        "id": "Pun40gVLtsf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapping the whole update into `update_fn` since it makes the updates much faster"
      ],
      "metadata": {
        "id": "QZeLtgeGR8uI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def update_fn(params, opt_state, x0, conditioning, rng):\n",
        "  grads, metrics = grad_fn(params, x0, conditioning, rng)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, metrics"
      ],
      "metadata": {
        "id": "_uXRZ2hSRjEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "e3ebC1hxIcVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nepochs = 15\n",
        "batch_size = 256\n",
        "epoch_size = 60000 // batch_size\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "params = network.initialize_variables(\n",
        "    input_shape=(1, 28, 28, 1, process.process_num_categories),\n",
        "    conditioning_shape={'label': (1,)},\n",
        "    key=rng,\n",
        "    is_training=True,\n",
        ")['params']"
      ],
      "metadata": {
        "id": "oRqMSxTmIeYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "train_iter = iter(mnist_dataset(batch_size, train=True))\n",
        "\n",
        "# --- 1. SETUP: Ensure Consistent Inputs ---\n",
        "# We fetch a fresh batch to ensure x0 and conditioning match in size.\n",
        "try:\n",
        "  batch = next(train_iter)\n",
        "except StopIteration:\n",
        "  print(\"Iterator exhausted. Please re-initialize 'train_iter'.\")\n",
        "  raise\n",
        "\n",
        "x0 = batch[\"data\"]\n",
        "conditioning = {\"label\": batch[\"label\"]}\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "print(f\"Benchmark Inputs Shape: {x0.shape}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# --- 2. Define Benchmark Functions ---\n",
        "\n",
        "\n",
        "# A. Isolate Data Corruption/Sampling\n",
        "@jax.jit\n",
        "def benchmark_corrupt(rng, x0):\n",
        "  time_rng, corrupt_rng = jax.random.split(rng, 2)\n",
        "  time_val = time_sampler(key=time_rng, data_spec=x0)\n",
        "  xt, targets = process.corrupt(key=corrupt_rng, x0=x0, time=time_val)\n",
        "  return xt, time_val, targets\n",
        "\n",
        "\n",
        "# B. Isolate Forward Pass Only\n",
        "# Note: We removed 'x0' from args as it is not used in this specific function\n",
        "@jax.jit\n",
        "def benchmark_forward(params, time_val, xt, conditioning, rng):\n",
        "  return network.apply(\n",
        "      {\"params\": params},\n",
        "      time=time_val,\n",
        "      xt=xt,\n",
        "      conditioning=conditioning,\n",
        "      is_training=True,\n",
        "      rngs={\"dropout\": rng},\n",
        "  )\n",
        "\n",
        "\n",
        "# C. Isolate Full Gradient Calculation\n",
        "@jax.jit\n",
        "def benchmark_grad(params, x0, conditioning, rng):\n",
        "  return grad_fn(params, x0, conditioning, rng)\n",
        "\n",
        "\n",
        "# --- 3. Run the Benchmark ---\n",
        "\n",
        "\n",
        "def block_any(tree):\n",
        "  \"\"\"Helper to block on the first available array in any structure.\"\"\"\n",
        "  jax.tree_util.tree_leaves(tree)[0].block_until_ready()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Part 1: Benchmarking Data Corruption\n",
        "# ---------------------------------------------------------\n",
        "print(\"1. Benchmarking Data Corruption...\")\n",
        "# Warmup\n",
        "res = benchmark_corrupt(rng, x0)\n",
        "block_any(res)\n",
        "\n",
        "t0 = time.time()\n",
        "for _ in range(10):\n",
        "  rng, subkey = jax.random.split(rng)\n",
        "  xt, t_val, targets = benchmark_corrupt(subkey, x0)\n",
        "  block_any(xt)\n",
        "avg_corrupt_time = (time.time() - t0) / 10.0\n",
        "print(f\"   Avg Time: {avg_corrupt_time:.5f} s\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Part 2: Benchmarking Network Forward Pass\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n2. Benchmarking Network Forward Pass...\")\n",
        "# Regenerate inputs\n",
        "rng, subkey = jax.random.split(rng)\n",
        "xt, t_val, targets = benchmark_corrupt(subkey, x0)\n",
        "block_any(xt)\n",
        "\n",
        "# Warmup\n",
        "res = benchmark_forward(params, t_val, xt, conditioning, rng)\n",
        "block_any(res)  # <--- FIXED: Handles dictionary return\n",
        "\n",
        "t0 = time.time()\n",
        "for _ in range(10):\n",
        "  rng, subkey = jax.random.split(rng)\n",
        "  out = benchmark_forward(params, t_val, xt, conditioning, subkey)\n",
        "  block_any(out)  # <--- FIXED\n",
        "avg_fwd_time = (time.time() - t0) / 10.0\n",
        "print(f\"   Avg Time: {avg_fwd_time:.5f} s\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Part 3: Benchmarking Full Gradient (Fwd + Bwd)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n3. Benchmarking Gradient Step (Forward + Backward)...\")\n",
        "# Warmup\n",
        "grads, metrics = benchmark_grad(params, x0, conditioning, rng)\n",
        "block_any(grads)\n",
        "\n",
        "t0 = time.time()\n",
        "for _ in range(10):\n",
        "  rng, subkey = jax.random.split(rng)\n",
        "  grads, metrics = benchmark_grad(params, x0, conditioning, subkey)\n",
        "  block_any(grads)\n",
        "avg_grad_time = (time.time() - t0) / 10.0\n",
        "print(f\"   Avg Time: {avg_grad_time:.5f} s\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Summary Analysis\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n\" + \"=\" * 40)\n",
        "print(\"PERFORMANCE BREAKDOWN\")\n",
        "print(\"=\" * 40)\n",
        "print(\n",
        "    f\"Data Corrupt:  {avg_corrupt_time:.5f} s \"\n",
        "    f\" ({(avg_corrupt_time/avg_grad_time)*100:.1f}%)\"\n",
        ")\n",
        "print(\n",
        "    f\"Forward Pass:  {avg_fwd_time:.5f} s     \"\n",
        "    f\" ({(avg_fwd_time/avg_grad_time)*100:.1f}%)\"\n",
        ")\n",
        "estimated_bwd = avg_grad_time - avg_fwd_time\n",
        "print(\n",
        "    f\"Backward Pass: {estimated_bwd:.5f} s \"\n",
        "    f\" ({(estimated_bwd/avg_grad_time)*100:.1f}%)\"\n",
        ")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Total Compute: {avg_grad_time + avg_corrupt_time:.5f} s\")"
      ],
      "metadata": {
        "id": "Ct17XmQC_PgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_state = optimizer.init(params)\n",
        "\n",
        "train_iter = iter(mnist_dataset(batch_size, train=True))\n",
        "\n",
        "losses = []\n",
        "\n",
        "nepochs = 15\n",
        "\n",
        "for epoch in tqdm.tqdm(range(1, nepochs + 1)):\n",
        "  epoch_loss = steps = 0\n",
        "  for i in range(epoch_size):\n",
        "    # Read batch of data\n",
        "    batch = next(train_iter)\n",
        "    # batch = batch_overfit\n",
        "    x0 = batch['data']\n",
        "    conditioning = {'label': batch['label']}\n",
        "    # Make the parameters update\n",
        "    rng, _ = jax.random.split(rng)\n",
        "    params, opt_state, metrics = update_fn(\n",
        "        params, opt_state, x0, conditioning, rng\n",
        "    )\n",
        "    epoch_loss += metrics['loss']\n",
        "    steps += 1\n",
        "  print(f'Epoch = {epoch}, Cumulative epoch loss = {epoch_loss}')\n",
        "  losses.append(epoch_loss)"
      ],
      "metadata": {
        "id": "boqKlkENt4aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)"
      ],
      "metadata": {
        "id": "Zt9AlILeuyQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9dX8jwMpfuv"
      },
      "source": [
        "# It's inference time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qij2alwWphef"
      },
      "source": [
        "Below, we define the inference function.\n",
        "It creates a pure jax function which takes `t`, `xt` and `c` to return the expected value of `x0`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_inference_fn = wrappers.FlaxLinenInferenceFn(\n",
        "    network=network,\n",
        "    params=params,\n",
        ")\n",
        "inference_fn = diffusion_inference.GuidedDiffusionInferenceFn(\n",
        "    base_inference_fn=base_inference_fn\n",
        ")"
      ],
      "metadata": {
        "id": "A8nH-A6pPkVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampler -- time_schedule, stepper and sampler itself"
      ],
      "metadata": {
        "id": "BcVHlXAhPuY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_sampling_steps = 28 * 28  # Number of denoising steps\n",
        "num_sampling_steps = 200\n",
        "time_schedule = time_scheduling.UniformTimeSchedule(safety_epsilon=1e-3)\n",
        "stepper = simplicial_step_sampler.SimplicialDDIMStep(corruption_process=process)\n",
        "\n",
        "sampler = sampling.DiffusionSampler(\n",
        "    time_schedule=time_schedule, stepper=stepper, num_steps=num_sampling_steps\n",
        ")\n",
        "sampler = functools.partial(sampler, inference_fn=inference_fn)\n",
        "sampler = jax.jit(jax.experimental.checkify.checkify(sampler))"
      ],
      "metadata": {
        "id": "sLpq6yoWw7rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling the data\n",
        "\n",
        "* First, we sample the data taking the conditioning from a batch of data, allowing to approximate $p(x_0)$\n",
        "\n",
        "* Second, we sample data with a given label, allowing to sample $p(x_0 | c)$"
      ],
      "metadata": {
        "id": "yTHo_XNGP0lU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_w_9y8OW8nz"
      },
      "outputs": [],
      "source": [
        "num_samples = 16\n",
        "data_spec = jnp.ones(\n",
        "    (num_samples, 28, 28, 1, process.process_num_categories), dtype=jnp.int32\n",
        ")\n",
        "specific_label = 5\n",
        "\n",
        "eval_iter = iter(mnist_dataset(num_samples, train=False))\n",
        "eval_data = next(eval_iter)\n",
        "\n",
        "################################################################################\n",
        "# Sample conditionally using dataset\n",
        "################################################################################\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "\n",
        "initial_noise = process.sample_from_invariant(key=key, data_spec=data_spec)\n",
        "conditioning = {\"label\": eval_data[\"label\"]}\n",
        "_, (out_cond, _) = sampler(\n",
        "    rng=key, initial_noise=initial_noise, conditioning=conditioning\n",
        ")\n",
        "\n",
        "################################################################################\n",
        "# Sample from a given label\n",
        "################################################################################\n",
        "\n",
        "key = jax.random.PRNGKey(1)\n",
        "initial_noise = process.sample_from_invariant(key=key, data_spec=data_spec)\n",
        "conditioning = {\n",
        "    \"label\": jnp.ones((num_samples,)).astype(jnp.int32) * specific_label\n",
        "}\n",
        "_, (out_label, _) = sampler(\n",
        "    rng=key, initial_noise=initial_noise, conditioning=conditioning\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize true dataset"
      ],
      "metadata": {
        "id": "ap-N0ng3VNfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cur_mnist_plot_images = eval_data['data']\n",
        "\n",
        "fig, axes = plt.subplots(2, 8, figsize=(8, 8))\n",
        "for img, ax in zip(cur_mnist_plot_images[:64], axes.flatten()):\n",
        "  ax.imshow(img[:, :, :, 0])\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GpNNzUxaVP41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize samples from $p(x_0)$"
      ],
      "metadata": {
        "id": "DmsEJpC2VQ1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cur_mnist_plot_images = out_cond.xt\n",
        "fig, axes = plt.subplots(2, 8, figsize=(8, 8))\n",
        "i = 0\n",
        "for img, ax in zip(cur_mnist_plot_images[:64], axes.flatten()):\n",
        "  ax.imshow(jnp.argmax(img, axis=-1))\n",
        "  ax.axis('off')\n",
        "  i += 1\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q7dxWHBPVPTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize samples from $p(x_0 | c)$"
      ],
      "metadata": {
        "id": "PEsoaSmsVVuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cur_mnist_plot_images = out_label.xt\n",
        "fig, axes = plt.subplots(2, 8, figsize=(8, 8))\n",
        "for img, ax in zip(cur_mnist_plot_images[:64], axes.flatten()):\n",
        "  ax.imshow(jnp.argmax(img, axis=-1))\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A4ljuY8cVVuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1yIgecGfN2TTVPLjbdNJrf32_jq95WDNq",
          "timestamp": 1767552707396
        }
      ],
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3_tpu",
        "kind": "private"
      }
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
