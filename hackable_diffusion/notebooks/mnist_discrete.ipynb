{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training MNIST using discrete diffusion ðŸš€\n",
        "\n",
        "In this colab we showcase how to train a **discrete** diffusion model on MNIST dataset. This colab can run on any colab backend."
      ],
      "metadata": {
        "id": "K43uX-gRu1O8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLydYq_yuo2I"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Common modules\n",
        "################################################################################\n",
        "\n",
        "import dataclasses\n",
        "import functools\n",
        "from etils import ecolab\n",
        "import flax.linen as nn\n",
        "import grain.python as pygrain\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow_datasets as tfds\n",
        "import tqdm\n",
        "\n",
        "################################################################################\n",
        "# Hackable diffusion modules\n",
        "################################################################################\n",
        "\n",
        "cell_autoreload = True  # @param{type: \"boolean\"}\n",
        "\n",
        "with ecolab.adhoc(\n",
        "    reload=[\"hackable_diffusion\"],\n",
        "    invalidate=False,\n",
        "    cell_autoreload=cell_autoreload,\n",
        "):\n",
        "  from hackable_diffusion import hd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diffusion_network = hd.diffusion_network\n",
        "time_sampling = hd.time_sampling\n",
        "discrete = hd.corruption.discrete\n",
        "schedules = hd.corruption.schedules\n",
        "arch_typing = hd.architecture.arch_typing\n",
        "conditioning_encoder = hd.architecture.conditioning_encoder\n",
        "discrete_backbone = hd.architecture.discrete\n",
        "unet = hd.architecture.unet\n",
        "wrappers = hd.inference.wrappers\n",
        "diffusion_inference = hd.inference.diffusion_inference\n",
        "discrete_loss = hd.loss.discrete\n",
        "discrete_step_sampler = hd.sampling.discrete_step_sampler\n",
        "sampling = hd.sampling.sampling\n",
        "time_scheduling = hd.sampling.time_scheduling"
      ],
      "metadata": {
        "id": "bnCZLUolDmbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare MNIST data"
      ],
      "metadata": {
        "id": "_e8om2QJvWDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create py-grain data structures for convenient batching and loading.\n",
        "\n",
        "MNIST data is $28 \\times 28 \\times 1$."
      ],
      "metadata": {
        "id": "kAeUl5SQpdGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclasses.dataclass(frozen=True)\n",
        "class PreprocessExample(pygrain.MapTransform):\n",
        "  \"\"\"Preprocesses an example.\"\"\"\n",
        "\n",
        "  def map(self, x: dict[str, np.ndarray]) -\u003e dict[str, np.ndarray]:\n",
        "    \"\"\"Converts everything to int32.\"\"\"\n",
        "\n",
        "    image = x['image'].astype(np.int32)\n",
        "    image = np.reshape(image, (28, 28, 1))\n",
        "    # We add additional dimension for the tokens\n",
        "    image = np.expand_dims(image, axis=-1)\n",
        "\n",
        "    return {\n",
        "        'data': image,\n",
        "        'label': np.int32(x['label']),\n",
        "    }\n",
        "\n",
        "\n",
        "def mnist_dataset(batch_size, train) -\u003e pygrain.DataLoader:\n",
        "  loader = pygrain.load(\n",
        "      source=tfds.data_source(name='mnist', split='all'),\n",
        "      shuffle=True if train else False,\n",
        "      shard_options=pygrain.ShardByJaxProcess(drop_remainder=True),\n",
        "      transformations=[PreprocessExample()],\n",
        "      batch_size=batch_size,\n",
        "      drop_remainder=True,\n",
        "      seed=0,\n",
        "  )\n",
        "  return loader"
      ],
      "metadata": {
        "id": "790dD3J2o5Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_plot_images = next(iter(mnist_dataset(64, train=False)))['data']\n",
        "fig, axes = plt.subplots(8, 8, figsize=(8, 8))\n",
        "for img, ax in zip(mnist_plot_images[:64], axes.flatten()):\n",
        "  ax.imshow(img[:, :, :, 0])\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6nYspS7cpM6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(linewidth=160)\n",
        "mnist_plot_images[0][:, :, 0, 0]"
      ],
      "metadata": {
        "id": "SpSzH10oZDtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define all diffusion model modules"
      ],
      "metadata": {
        "id": "OWP4NiIfMBde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noise process"
      ],
      "metadata": {
        "id": "xN5dcCQhLRSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use cosine discrete schedule"
      ],
      "metadata": {
        "id": "odHo5roqFfYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schedule = schedules.CosineDiscreteSchedule()\n",
        "process = discrete.CategoricalProcess.masking_process(\n",
        "    schedule=schedule, num_categories=256\n",
        ")"
      ],
      "metadata": {
        "id": "doItCfTdLQvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "invariant_probs_masking = (0.0,) * process.num_categories + (1.0,)\n",
        "process.invariant_probs == invariant_probs_masking"
      ],
      "metadata": {
        "id": "k53BAOuXy4LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process.is_masking"
      ],
      "metadata": {
        "id": "NWurTa1UzIES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize noise process"
      ],
      "metadata": {
        "id": "SDjo5LRSLskH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_noises = 7\n",
        "fig, axes = plt.subplots(\n",
        "    ncols=num_noises, figsize=(num_noises * 4, 4), sharex=True, sharey=True\n",
        ")\n",
        "\n",
        "corrupt_rng = jax.random.PRNGKey(10)\n",
        "idx = 0\n",
        "for time in jnp.linspace(1e-3, 1.0 - 1e-3, num=num_noises):\n",
        "  xt, targets = process.corrupt(\n",
        "      key=corrupt_rng,\n",
        "      x0=jnp.array(mnist_plot_images),\n",
        "      time=jnp.ones((1,)) * time,\n",
        "  )\n",
        "  ax = axes[idx]\n",
        "  ax.imshow(xt[0, :, :, :, 0])\n",
        "  ax.axis('off')\n",
        "  ax.set_title(f'Time = {time}')\n",
        "  idx += 1"
      ],
      "metadata": {
        "id": "12mocRWzLuAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define diffusion network backbone\n",
        "\n",
        "First, we define diffusion backbone -- an architecture which takex `x` and `conditioning_embeddings`, as well as `is_training` and returns the same type as `x`.\n",
        "\n",
        "Here, we use a small version of `Unet`."
      ],
      "metadata": {
        "id": "8fiOvaxNMOh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_backbone = unet.Unet(\n",
        "    base_channels=32,\n",
        "    channels_multiplier=(1, 2, 2),\n",
        "    num_residual_blocks=(2, 2, 2),\n",
        "    downsample_method=arch_typing.DownsampleType.AVG_POOL,\n",
        "    upsample_method=arch_typing.UpsampleType.NEAREST,\n",
        "    dropout_rate=(0.0, 0.0, 0.2),\n",
        "    bottleneck_dropout_rate=0.2,\n",
        "    self_attention_bool=(False, False, False),\n",
        "    cross_attention_bool=(False, False, False),\n",
        "    attention_normalize_qk=False,\n",
        "    attention_use_rope=False,\n",
        "    attention_rope_position_type=arch_typing.RoPEPositionType.SQUARE,\n",
        "    attention_num_heads=8,\n",
        "    attention_head_dim=-1,\n",
        "    normalization_type=arch_typing.NormalizationType.RMS_NORM,\n",
        "    normalization_num_groups=None,\n",
        "    zero_init_output=False,\n",
        "    activation='gelu',\n",
        "    skip_connection_method=arch_typing.SkipConnectionMethod.NORMALIZED_ADD,\n",
        ")\n",
        "\n",
        "backbone = discrete_backbone.ConditionalDiscreteBackbone(\n",
        "    base_backbone=base_backbone,\n",
        "    token_embedder=discrete_backbone.TokenEmbedder(\n",
        "        process_num_categories=process.process_num_categories,\n",
        "        embedding_dim=32,\n",
        "        adapt_to_image_like_data=True,\n",
        "    ),\n",
        "    token_projector=discrete_backbone.DenseTokenProjector(\n",
        "        embedding_dim=32,\n",
        "        num_categories=256,\n",
        "        adapt_to_image_like_data=True,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "11x4iOOprtTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define conditioning logic"
      ],
      "metadata": {
        "id": "ARc5ksUEHG0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we define the conditioning embedders as well as the time encoder. The conditioning encoder processes each conditioning (in the case of MNIST data, each batch comes with its label (`label`)).\n",
        "\n",
        "The conditioning encoder is a dictionary with key `label` (and here the value is a `nn.Module` which is given by a simple `LabelEmbedding` module). If you want to train a purely unconditional model, set `conditioning_embedders = {}`.\n",
        "\n"
      ],
      "metadata": {
        "id": "fqGG4Co-HD1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Conditional diffusion.\n",
        "################################################################################\n",
        "\n",
        "conditioning_embedders = {\n",
        "    'label': conditioning_encoder.LabelEmbedder(\n",
        "        num_classes=10,\n",
        "        num_features=256,\n",
        "        conditioning_key='label',\n",
        "    )\n",
        "}\n",
        "\n",
        "encoder = conditioning_encoder.ConditioningEncoder(\n",
        "    time_embedder=conditioning_encoder.SinusoidalTimeEmbedder(\n",
        "        activation='gelu', embedding_dim=256, num_features=256\n",
        "    ),\n",
        "    conditioning_embedders=conditioning_embedders,\n",
        "    embedding_merging_method=arch_typing.EmbeddingMergeMethod.SUM,\n",
        "    conditioning_rules={\n",
        "        'time': arch_typing.ConditioningMechanism.ADAPTIVE_NORM,\n",
        "        'label': arch_typing.ConditioningMechanism.ADAPTIVE_NORM,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "pDQv0FEEHFj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting all together into diffusion network"
      ],
      "metadata": {
        "id": "E9Cy-hgrH7gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = diffusion_network.DiffusionNetwork(\n",
        "    backbone_network=backbone,\n",
        "    conditioning_encoder=encoder,\n",
        "    prediction_type='logits',\n",
        "    data_dtype=jnp.int32,\n",
        ")"
      ],
      "metadata": {
        "id": "7EUVYISMH9bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model visualization"
      ],
      "metadata": {
        "id": "5rwfPG_EtTS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_depth = 2  # @param {type: \"integer\"}\n",
        "\n",
        "tabulate_fn = nn.tabulate(\n",
        "    network,\n",
        "    jax.random.PRNGKey(42),\n",
        "    depth=summary_depth,\n",
        "    console_kwargs={\"force_jupyter\": True, \"soft_wrap\": True},\n",
        ")\n",
        "\n",
        "dummy_time = jnp.ones((1,))\n",
        "dummy_xt = jnp.ones((1, 28, 28, 1, 1), dtype=jnp.int32)\n",
        "dummy_conditioning = {\"label\": jnp.ones((1,), dtype=jnp.int32)}\n",
        "\n",
        "print(\n",
        "    tabulate_fn(\n",
        "        dummy_time,\n",
        "        dummy_xt,\n",
        "        dummy_conditioning,\n",
        "        is_training=False,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "LvX6WLQGN1cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define time sampler, optimizer and loss function\n",
        "\n",
        "The time is sampled uniformly in the interval $[\\epsilon,1 - \\epsilon]$.\n",
        "\n",
        "The loss is simply the $\\ell_2$ loss."
      ],
      "metadata": {
        "id": "emGbkd8vMWyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_sampler = time_sampling.UniformTimeSampler(safety_epsilon=1e-3)\n",
        "\n",
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(max_norm=1.0),\n",
        "    optax.scale_by_adam(b1=0.9, b2=0.999, eps=1e-8),\n",
        "    optax.scale_by_schedule(optax.constant_schedule(value=5e-4)),\n",
        "    optax.scale(-1.0),\n",
        ")\n",
        "\n",
        "loss_fn = discrete_loss.DiffusionCrossEntropyLoss(schedule=schedule)"
      ],
      "metadata": {
        "id": "1Y-S0iHuIa9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the parameters loss function and gradient function\n",
        "\n",
        "Here we define the loss function as well as gradient function to be dependent on NN parameters. This is needed for training the neural network."
      ],
      "metadata": {
        "id": "qYyGqTogMdp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def params_loss_fn(params, x0, conditioning, rng):\n",
        "  time_rng, corrupt_rng = jax.random.split(rng, 2)\n",
        "  time = time_sampler(key=time_rng, data_spec=x0)\n",
        "  xt, targets = process.corrupt(key=corrupt_rng, x0=x0, time=time)\n",
        "  output = network.apply(\n",
        "      {'params': params},\n",
        "      time=time,\n",
        "      xt=xt,\n",
        "      conditioning=conditioning,\n",
        "      is_training=True,\n",
        "      rngs={'dropout': rng},\n",
        "  )\n",
        "  out = jnp.mean(loss_fn(preds=output, targets=targets, time=time))\n",
        "  return out, {'loss': out}\n",
        "\n",
        "\n",
        "grad_fn = jax.jit(jax.grad(params_loss_fn, has_aux=True))"
      ],
      "metadata": {
        "id": "Pun40gVLtsf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapping the whole update into `update_fn` since it makes the updates much faster"
      ],
      "metadata": {
        "id": "QZeLtgeGR8uI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def update_fn(params, opt_state, x0, conditioning, rng):\n",
        "  grads, metrics = grad_fn(params, x0, conditioning, rng)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, metrics"
      ],
      "metadata": {
        "id": "_uXRZ2hSRjEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "e3ebC1hxIcVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nepochs = 15\n",
        "batch_size = 256\n",
        "epoch_size = 60000 // batch_size\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "params = network.initialize_variables(\n",
        "    input_shape=(1, 28, 28, 1, 1),\n",
        "    conditioning_shape={'label': (1,)},\n",
        "    key=rng,\n",
        "    is_training=True,\n",
        ")['params']"
      ],
      "metadata": {
        "id": "oRqMSxTmIeYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_state = optimizer.init(params)\n",
        "\n",
        "train_iter = iter(mnist_dataset(batch_size, train=True))\n",
        "\n",
        "losses = []\n",
        "for epoch in tqdm.tqdm(range(1, nepochs + 1)):\n",
        "  epoch_loss = steps = 0\n",
        "  for i in range(epoch_size):\n",
        "    # Read batch of data\n",
        "    batch = next(train_iter)\n",
        "    x0 = batch['data']\n",
        "    conditioning = {'label': batch['label']}\n",
        "    # Make the parameters update\n",
        "    rng, _ = jax.random.split(rng)\n",
        "    params, opt_state, metrics = update_fn(\n",
        "        params, opt_state, x0, conditioning, rng\n",
        "    )\n",
        "    epoch_loss += metrics['loss']\n",
        "    steps += 1\n",
        "  print(f'Epoch = {epoch}, Cumulative epoch loss = {epoch_loss}')\n",
        "  losses.append(epoch_loss)"
      ],
      "metadata": {
        "id": "boqKlkENt4aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)"
      ],
      "metadata": {
        "id": "Zt9AlILeuyQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9dX8jwMpfuv"
      },
      "source": [
        "# It's inference time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qij2alwWphef"
      },
      "source": [
        "Below, we define the inference function.\n",
        "It creates a pure jax function which takes `t`, `xt` and `c` to return the expected value of `x0`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_inference_fn = wrappers.FlaxLinenInferenceFn(\n",
        "    network=network,\n",
        "    params=params,\n",
        ")\n",
        "inference_fn = diffusion_inference.GuidedDiffusionInferenceFn(\n",
        "    base_inference_fn=base_inference_fn\n",
        ")"
      ],
      "metadata": {
        "id": "A8nH-A6pPkVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampler -- time_schedule, stepper and sampler itself"
      ],
      "metadata": {
        "id": "BcVHlXAhPuY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_sampling_steps = 28 * 28  # Number of denoising steps\n",
        "time_schedule = time_scheduling.UniformTimeSchedule()\n",
        "stepper = discrete_step_sampler.UnMaskingStep(corruption_process=process)\n",
        "\n",
        "sampler = sampling.DiffusionSampler(\n",
        "    time_schedule=time_schedule, stepper=stepper, num_steps=num_sampling_steps\n",
        ")\n",
        "sampler = functools.partial(sampler, inference_fn=inference_fn)\n",
        "sampler = jax.jit(jax.experimental.checkify.checkify(sampler))"
      ],
      "metadata": {
        "id": "sLpq6yoWw7rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling the data\n",
        "\n",
        "* First, we sample the data taking the conditioning from a batch of data, allowing to approximate $p(x_0)$\n",
        "\n",
        "* Second, we sample data with a given label, allowing to sample $p(x_0 | c)$"
      ],
      "metadata": {
        "id": "yTHo_XNGP0lU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_w_9y8OW8nz"
      },
      "outputs": [],
      "source": [
        "num_samples = 64\n",
        "data_spec = jnp.ones((num_samples, 28, 28, 1, 1), dtype=jnp.int32)\n",
        "specific_label = 5\n",
        "\n",
        "eval_iter = iter(mnist_dataset(num_samples, train=False))\n",
        "eval_data = next(eval_iter)\n",
        "\n",
        "################################################################################\n",
        "# Sample conditionally using dataset\n",
        "################################################################################\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "\n",
        "initial_noise = process.sample_from_invariant(key=key, data_spec=data_spec)\n",
        "conditioning = {\"label\": eval_data[\"label\"]}\n",
        "_, (out_cond, _) = sampler(\n",
        "    rng=key, initial_noise=initial_noise, conditioning=conditioning\n",
        ")\n",
        "\n",
        "################################################################################\n",
        "# Sample from a given label\n",
        "################################################################################\n",
        "\n",
        "key = jax.random.PRNGKey(1)\n",
        "initial_noise = process.sample_from_invariant(key=key, data_spec=data_spec)\n",
        "conditioning = {\n",
        "    \"label\": jnp.ones((num_samples,)).astype(jnp.int32) * specific_label\n",
        "}\n",
        "_, (out_label, _) = sampler(\n",
        "    rng=key, initial_noise=initial_noise, conditioning=conditioning\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize true dataset"
      ],
      "metadata": {
        "id": "ap-N0ng3VNfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cur_mnist_plot_images = eval_data['data']\n",
        "fig, axes = plt.subplots(8, 8, figsize=(8, 8))\n",
        "for img, ax in zip(cur_mnist_plot_images[:64], axes.flatten()):\n",
        "  ax.imshow(img[:, :, :, 0])\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GpNNzUxaVP41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize samples from $p(x_0)$"
      ],
      "metadata": {
        "id": "DmsEJpC2VQ1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(linewidth=160)\n",
        "eval_data['data'][0][:, :, :, 0].reshape((28, 28))"
      ],
      "metadata": {
        "id": "ZqL0fQ6Av2ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cur_mnist_plot_images = out_cond.xt\n",
        "fig, axes = plt.subplots(8, 8, figsize=(8, 8))\n",
        "for img, ax in zip(cur_mnist_plot_images[:64], axes.flatten()):\n",
        "  ax.imshow(img[:, :, :, 0])\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q7dxWHBPVPTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(linewidth=160)\n",
        "out_cond.xt[0][:, :, :, 0].reshape((28, 28))"
      ],
      "metadata": {
        "id": "otTco32d0oc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize samples from $p(x_0 | c)$"
      ],
      "metadata": {
        "id": "PEsoaSmsVVuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cur_mnist_plot_images = out_label.xt\n",
        "fig, axes = plt.subplots(8, 8, figsize=(8, 8))\n",
        "for img, ax in zip(cur_mnist_plot_images[:64], axes.flatten()):\n",
        "  ax.imshow(img[:, :, :, 0])\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A4ljuY8cVVuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(linewidth=160)\n",
        "out_label.xt[0][:, :, :, 0].reshape((28, 28))"
      ],
      "metadata": {
        "id": "gtXXIuyv63ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HcgXMmPt7RLz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3_tpu",
        "kind": "private"
      }
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
